from collections import defaultdict
from Bio import SeqIO
from Bio import SearchIO
from Bio.SeqUtils import GC
import pandas as pd
import argparse
import subprocess
import re
import glob
import gzip

parser = argparse.ArgumentParser()
#Index and filtering options
parser.add_argument("--rename", help="Flag to rename sequences according to file identifier", default=False, type=bool)
parser.add_argument("--min_scaffold_length", help="Minimum scaffold length", default=2500, type=int)
parser.add_argument("--assemblies_files", help="Fasta files of assembled contigs", nargs="+", type=str)
parser.add_argument("--assembly_format", help="Assembly files format", default="fasta", type=str)
#Assembly options
parser.add_argument("--assemble", help="Flag to run the assembly module", default=False, type=bool)
parser.add_argument("--coassembly_table", help="Coassembly instructions .tsv table", type=str)
parser.add_argument("--max_memory", help="Maximum memory to use durign assembly(GB)", default=500, type=int)
#Contig/Scaffold abundance options
parser.add_argument("--calc_abundance", help="Flag to run the abundance calculation modules", default=False, type=bool)
parser.add_argument("--bowtie2_db", help="Bowtie DB file to be used for calculating contig abudnances", type=str)
parser.add_argument("--tax_aware_binning", help="Flag to perform taxonomy aware binning module", default=False, type=bool)
parser.add_argument("--tax_level", help="Taxonomic level to split the sequences for tax aware binning", default="phylum", type=str)
#Contig/Scaffold Taxonomic annotation Options
parser.add_argument("--cat_db", help="Path to cat db", default="/mnt/lustre/repos/bio/databases/public/cat/20210107/2021-01-07_CAT_database", type=str)
parser.add_argument("--cat_taxonomy", help="Path to cat taxonomy", default="/mnt/lustre/repos/bio/databases/public/cat/20210107/2021-01-07_taxonomy", type=str)
#Binning options
parser.add_argument("--make_bins", help="Flag to run the binning module", default=False, type=bool)
parser.add_argument("--binning_method", help="Binning method to use", default='metabat', type=str)
parser.add_argument("--refine_bins", help="Flag to run the Bin refinement module", default=False, type=bool)
parser.add_argument("--refinement_contamination_cutoff", help="Minimum contamination of bins to be refined by splitting contigs", default=5, type=float)
parser.add_argument("--refinement_tax_level", help="Taxonomic level for splitting contigs during bin refinement", default="class", type=str)
parser.add_argument("--jgi_summary", help="Optional table generated by jgi_summarize_bam_contig_depths", type=str)
#Bin QC options
parser.add_argument("--call_checkm", help="Flag to run CheckM", default=False, type=bool)
parser.add_argument("--call_GTDBtk", help="Flag to run GTDBtk", default=False, type=bool)
#Output options
parser.add_argument("--scaffold_info_table", help=".tsv file to write scaffold info", default="Scaffolds_Info.tsv", type=str)
parser.add_argument("--bin_info_table", help=".tsv file to write Bin info", default="Bin_Info.tsv", type=str)
#Processing options
parser.add_argument("--threads", help="The number of threads to be used", default=1, type=int)
parser.add_argument("--parse_only", help="Flag to skip running any programs and only parse their output", default=False, type=bool)

args = parser.parse_args()


def central():
    bin_info = defaultdict(dict)
    seq_info = defaultdict(dict)
    jgi_summary = args.jgi_summary
    #Perform the assemblies if specified by the user
    coassembly_df = pd.DataFrame({'DUMMY' : []})
    if (args.coassembly_table):
        print(f"Reading coassembly info from {args.coassembly_table}")
        coassembly_df = pd.read_csv(args.coassembly_table, sep="\t",index_col=0,header=0)
    assemblies_list = args.assemblies_files
    if (args.assemble == True):
        assemblies_list = assembly_module(coassembly_df = coassembly_df, threads = args.threads, max_memory = args.max_memory)
    #Index assembled seq info
    (seq_info,assemblies_list) = index_assemblies(assemblies_list = assemblies_list, rename_seqs = args.rename ,  min_length = args.min_scaffold_length)
    if (args.calc_abundance == True):
        (contig_abundances, seq_info, jgi_summary) = abundance_module(assemblies_list = assemblies_list, seq_info = seq_info,coassembly_df = coassembly_df, db = args.bowtie2_db, make_summary = True)
    if ((args.tax_aware_binning) or (args.refine_bins)):
        (seq_info,assemblies_list) = annotation_module(assemblies_list = assemblies_list, seq_info = seq_info)
    if (args.make_bins):
        (seq_info,bin_info) = binning_module(assemblies_files_list = assemblies_list, threads = args.threads, method = args.binning_method, min_length = args.min_scaffold_length, seq_info = seq_info, jgi_summary = jgi_summary)
    if (args.call_checkm):
        (bin_info) = checkm_module(bin_info = bin_info)
    if (args.refine_bins):
        (seq_info,bin_info) = refinement_module(seq_info = seq_info,bin_info = bin_info, level = args.refinement_tax_level)
    #Convert the 2d dictionary info_dict into a pandas dataframe and print it to output_dataframe_file in .tsv format
    print(f"Printing scaffold info to {args.scaffold_info_table}")
    info_dataframe = pd.DataFrame.from_dict(seq_info)
    info_dataframe.index.name = 'Scaffold'
    info_dataframe.to_csv(args.scaffold_info_table,sep="\t",na_rep='NA')
    
    print(f"Printing Bin info to {args.bin_info_table}")
    info_dataframe = pd.DataFrame.from_dict(bin_info)
    info_dataframe.index.name = 'Bin'
    info_dataframe.to_csv(args.bin_info_table,sep="\t",na_rep='NA')

def abundance_module(assemblies_list = None, seq_info = None, coassembly_df = None, db = None):
    print("Running abundance module")
    contig_abundances = defaultdict(dict)
    
    for assembly_file in assemblies_list:
        print(f"Processing {assembly_file}")
        prefix_assembly_file = get_prefix(assembly_file,"fasta")
        raw_abund_matrix = pd.DataFrame()
        
        if (db):
            print (f"Using predefined Bowtie2 DB {db}")
        else:
            print(f'Building Bowtie2 database from {assembly_file}')
            command = f'bowtie2-build {assembly_file} {prefix_assembly_file}'
            db_file = prefix_assembly_file
            db_file_prefix = prefix_assembly_file
            subprocess.call(command, shell=True)
        
        
        for sample,row in coassembly_df.iterrows():
            print(f'Aligning reads of {sample}')
            outfile = sample+'x'+db_file_prefix
            if ((row['R1'] == True) and  (row['R2'] == True)):
                r1_file = row['R1']
                r2_file = row['R2']
                command = f"bowtie2 -x {db_file} -q -1 {r1_file} -2 {r2_file} -S {outfile}.sam --sensitive-local --no-unal --threads {args.threads}"
                subprocess.call(command, shell=True)
            else:
                r1_file = row['R1']
                command = f"bowtie2 -x {db_file} -q -U {r1_file} -S {outfile}.sam  --sensitive-local --no-unal --threads {args.threads}"
                subprocess.call(command, shell=True)
            command = f'samtools view -bS {outfile}.sam > {outfile}.bam'
            subprocess.call(command, shell=True)
            command = f'samtools sort {outfile}.bam -o {outfile}.sorted.bam'
            subprocess.call(command, shell=True)
            command = f'rm -f {outfile}.sam {outfile}.bam'
            subprocess.call(command, shell=True)
            command = f'samtools index {outfile}.sorted.bam'
            subprocess.call(command, shell=True)
            command = f'samtools idxstats {outfile}.sorted.bam > {outfile}.Counts.tsv'
            subprocess.call(command, shell=True)
            sample_abund = index_info(table_file = f'{outfile}.Counts.tsv',index_col_name = None,sep_var = '\t',header=None)
            sample_abund = sample_abund.rename(columns={0: "Sequence", 1: "Length",2: sample, 3: "Unmapped"})
            sample_abund = sample_abund.set_index('Sequence')
            sample_abund.drop(sample_abund.tail(1).index,inplace=True)
            frames = [raw_abund_matrix,sample_abund[sample]]
            raw_abund_matrix = pd.concat(frames,axis=1)
    jgi_summary = None
    if (args.jgi_summary):
        jgi_summary = args.jgi_summary
    else:
        jgi_summary = f"JGI_Depth.tsv"
        command = (f"jgi_summarize_bam_contig_depths --outputDepth {jgi_summary} *bam")

    raw_abund_matrix_file = 'Raw_Abundance.tsv'
    raw_abund_matrix.to_csv(raw_abund_matrix_file,sep="\t",na_rep='NA')
    return(contig_abundances,seq_info,jgi_summary)

def annotation_module(assemblies_list = "NA", seq_info = "NA"):
    split_assemblies_files = []
    for assembly_file in assemblies_list:
        assembly_file_prefix = get_prefix(assembly_file,"fasta")
        command = f"CAT contigs -c {assembly_file} -d {args.cat_db} -t {args.cat_taxonomy} --nproc {args.threads} --no_stars --fraction 0.05 --force"
        if (args.parse_only == False):
            print(f"Running: {command}")
            subprocess.call(command, shell=True)
        if (args.parse_only == False):
            command = f"CAT add_names -i out.CAT.contig2classification.txt -o Taxonomy_out.CAT.contig2classification.txt -t {args.cat_taxonomy} --force"
            print(f"Running: {command}")
            subprocess.call(command, shell=True)
            command = f"mv Taxonomy_out.CAT.contig2classification.txt {assembly_file_prefix}_Taxonomy_out.CAT.contig2classification.txt"
            subprocess.call(command, shell=True)
        print("Processing CAT output file")
        with open(f"{assembly_file_prefix}_Taxonomy_out.CAT.contig2classification.txt","r") as INPUT:
            valid_ranks = ["superkingdom","phylum","class","order","family","genus","species"]
            ranks_re = re.compile("(superkingdom)|(phylum)|(class)|(order)|(family)|(genus)|(species)")
            par_re = re.compile("(\()|(\))|(\:)")
            header = INPUT.readline()
            for tline in INPUT.readlines():
                tline = tline.strip()
                fields = tline.split("\t")
                #print(fields)
                for field in fields:
                    if (re.search(ranks_re,field) != None):
                        #print(field,"matched")
                        values = field.split(" ")
                        taxon = values[0]
                        rank = values[1]
                        rank_score = values[2]
                        rank = re.sub(par_re,"",rank)
                        if ((rank in valid_ranks) and (fields[0] not in seq_info[rank])):
                            #print(fields[0],rank,taxon)
                            seq_info[rank][fields[0]] = taxon
                            seq_info[rank+"_Score"][fields[0]] = rank_score
        new_assemblies_list = split_sequences(assembly_file = assembly_file,seq_info=seq_info,level = args.tax_level)
        split_assemblies_files = split_assemblies_files + new_assemblies_list
    return(seq_info,split_assemblies_files)


def checkm_module(bin_info = defaultdict(dict)):
    command = f"checkm lineage_wf --tab_table --file CheckM_Bin_Info.tsv --threads {args.threads} --pplacer_threads {args.threads} --extension fa . CheckM_Results"
    if (args.parse_only == False):
        print(f"Running: {command}")
        subprocess.call(command, shell=True)
    checkm_df =  pd.read_csv("CheckM_Bin_Info.tsv", sep="\t",index_col=0,header=0)
    for i,row in checkm_df.iterrows():
        bin_info['Marker_Lineage'][i] = row['Marker lineage']
        bin_info['Completeness'][i] = row['Completeness']
        bin_info['Contamination'][i] = row['Contamination']
        bin_info['Strain_Heterogeneity'][i] = row['Strain heterogeneity']
    return(bin_info)

def index_bins(prefix="Bin",extension="fa",file_format="fasta",seq_info = defaultdict(dict)):
    bin_info = defaultdict(dict)
    bin_files = glob.glob(f"{prefix}*{extension}")
    print ('Indexing bins')
    for bin_file in bin_files:
        print(f'Processing Bin {bin_file}')
        bin_file_prefix = get_prefix(bin_file,"fa")
        bin_info['Sequences'][bin_file_prefix] = 0
        bin_info['Base_Pairs'][bin_file_prefix] = 0
        bin_info['File'][bin_file_prefix] = bin_file
        #Iterate over genomic sequences in the file. Collect basic Info
        for seqobj in SeqIO.parse(bin_file, "fasta"):
            bin_info['Sequences'][bin_file_prefix] += 1
            bin_info['Base_Pairs'][bin_file_prefix] += len(seqobj.seq)
            seq_info['Bin'][seqobj.id] = bin_file_prefix
            seq_info['Bin_File'][seqobj.id] = bin_file
                
    return(seq_info,bin_info)

def index_info(table_file = None ,index_col_name = None,sep_var='\t',header='infer'):
    print(f'Reading info from {table_file}')
    info_data_frame = pd.read_csv(table_file, sep=sep_var,index_col=index_col_name,header=header)
    return info_data_frame    
    
def refinement_module(seq_info = defaultdict(dict),bin_info = defaultdict(dict), level="class"):
    print("Running Bin refinment module")
    for bin_id in bin_info['Completeness']:
        if (bin_info['Contamination'][bin_id] >= args.refinement_contamination_cutoff):
            print(f"Cleaning bin {bin_id}")
            for seqobj in SeqIO.parse(f"{bin_id}.fa", "fasta"):
                taxon = str(seq_info[level].get(seqobj.id,"Unclassified"))
                split_file_name = f"Refined_{bin_id}_{taxon}.fa"
                bin_file_prefix = f"Refined_{bin_id}_{taxon}"
                with open(split_file_name, 'a', newline='') as OUT:
                    seq_info["Refined_Bin"][seqobj.id] = bin_file_prefix
                    seq_info["Refined_Bin_File"][seqobj.id] = split_file_name
                    if (bin_file_prefix not in bin_info['Sequences']):
                        bin_info['Sequences'][bin_file_prefix] = 0
                        bin_info['Base_Pairs'][bin_file_prefix] = 0
                    bin_info['Sequences'][bin_file_prefix] += 1
                    bin_info['Base_Pairs'][bin_file_prefix] += len(seqobj.seq)
                    SeqIO.write(seqobj, OUT, "fasta")
    return(seq_info,bin_info)

def split_sequences(assembly_file = "NA" , seq_info=defaultdict(dict),level = "phylum"):
    print(f"Splitting sequences from {assembly_file} according to:",level)
    assembly_file_prefix = get_prefix(assembly_file,"fasta")
    split_assemblies_files = []
    for seqobj in SeqIO.parse(assembly_file, "fasta"):
        taxon = str(seq_info[level].get(seqobj.id,"Unclassified"))
        split_file_name = f"Split_{assembly_file_prefix}_{taxon}.fasta"
        if (split_file_name not in split_assemblies_files):
            split_assemblies_files.append(split_file_name)
            #print("New split file:",split_file_name)
        with open(split_file_name, 'a', newline='') as OUT:
            SeqIO.write(seqobj, OUT, "fasta")
    #print("Split files:",split_assemblies_files)
    #print(type(split_assemblies_files))
    return(split_assemblies_files)



def binning_module(seq_info = None, min_length = 2000, assemblies_files_list = None, threads = 1, method = "metabat", jgi_summary = None):
    if (method == "metabat"):
        for assembly_file in list(assemblies_files_list):
            print("Binning file:",assembly_file)
            assembly_file_prefix = get_prefix(assembly_file,"fasta")
            command = f"metabat --inFile {assembly_file} --outFile Bin_{assembly_file_prefix} --minContig {min_length} --numThreads {threads}"
            if (jgi_summary):
                command = command + f" --abdFile {jgi_summary}"
            if (args.parse_only == False):
                print("Running",command)
                subprocess.call(command, shell=True)
    (seq_info,bin_info) = index_bins(prefix="Bin",extension="fa",file_format="fasta",seq_info = seq_info)
    return(seq_info,bin_info)
            
def assembly_module(coassembly_df = False, threads = 1, max_memory = 500):
    assemblies_list = []
    if (not coassembly_df.empty):
        for group in set(coassembly_df['Group']):
            print(f"Processing samples from group {group}")
            group_df = coassembly_df[coassembly_df['Group'] == group]
            samples_list = group_df.index
            print(f"Samples {samples_list}")
            r1_files = list(group_df['R1'])
            r1_files = ' -1 '.join(r1_files)
            r2_files = list(group_df['R2'])
            r2_files = ' -2 '.join(r2_files)
            #print(r1_files,r2_files)
            command = f"spades.py --meta --memory {max_memory} --threads {threads} -1 {r1_files} -2 {r2_files} -o Assembly_Group_{group}"
            if (args.parse_only == False):
                print(f"Running: {command}")
                subprocess.call(command, shell=True)
    return assemblies_list

def index_assemblies(assemblies_list = [], rename_seqs = False, min_length = 2000):
    seq_info = defaultdict(dict)
    filtered_assemblies_list = []
    print ('Indexing scaffolds')
    for assembly_file in assemblies_list:
        print(f'Processing assembly {assembly_file}')
        assembly_file_prefix = get_prefix(assembly_file,"fasta")
        out_seq_file = "Filtered_Renamed_"+assembly_file_prefix+'.fasta'
        filtered_assemblies_list.append(out_seq_file)
        with open(out_seq_file, 'w', newline='') as OUT:
            seq_counter = 0
            #Iterate over genomic sequences in the file. Collect basic Info
            for seqobj in SeqIO.parse(assembly_file, "fasta"):
                seq_counter += 1
                #Rename sequences if specified in the function call
                if (rename_seqs == True):
                    new_id = assembly_file_prefix+'_Scaffold_'+str(seq_counter)
                    seq_info['Original_ID'][new_id] = seqobj.id
                    seqobj.id = new_id    
                    #Do not allow duplicated IDs
                    if (seqobj.id in seq_info['Description']):
                         raise Exception(f'Duplicated ID: {seqobj.id} in {assembly_file}')
                seq_info['Description'][seqobj.id] = seqobj.description
                seq_info['GC'][seqobj.id] = round(GC(seqobj.seq),2)
                seq_info['Length'][seqobj.id] = len(seqobj.seq)
                seq_info['Original_File'][seqobj.id] = assembly_file
                if (seq_info['Length'][seqobj.id] >= min_length):
                    SeqIO.write(seqobj, OUT, "fasta")
            
    if (rename_seqs == True):
        return (seq_info,filtered_assemblies_list)
    else:
        return (seq_info,assemblies_list)
    
def get_prefix(file,file_format):
    prefix_file = re.sub(f".{file_format}","",file)
    prefix_file = re.sub("(.)+/","",prefix_file)
    return prefix_file
    
central()